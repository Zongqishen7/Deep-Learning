{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict (before training) 4 4.0\n",
      "\t grad: 1.0 2.0 -2.0\n",
      "\t grad: 2.0 3.0 -3.8400001525878906\n",
      "\t grad: 3.0 4.0 -4.948800086975098\n",
      "progress: 0 0.6802950501441956 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.784224033355713\n",
      "\t grad: 2.0 3.0 -2.9941577911376953\n",
      "\t grad: 3.0 4.0 -3.197906970977783\n",
      "progress: 1 0.2840724587440491 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.6246981620788574\n",
      "\t grad: 2.0 3.0 -2.368816375732422\n",
      "\t grad: 3.0 4.0 -1.9034500122070312\n",
      "progress: 2 0.10064227879047394 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.506758689880371\n",
      "\t grad: 2.0 3.0 -1.906494140625\n",
      "\t grad: 3.0 4.0 -0.9464435577392578\n",
      "progress: 3 0.02488209493458271 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.419564962387085\n",
      "\t grad: 2.0 3.0 -1.5646944046020508\n",
      "\t grad: 3.0 4.0 -0.23891830444335938\n",
      "progress: 4 0.0015856098616495728 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.3551013469696045\n",
      "\t grad: 2.0 3.0 -1.311997413635254\n",
      "\t grad: 3.0 4.0 0.2841653823852539\n",
      "progress: 5 0.0022430545650422573 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.3074429035186768\n",
      "\t grad: 2.0 3.0 -1.1251764297485352\n",
      "\t grad: 3.0 4.0 0.6708869934082031\n",
      "progress: 6 0.012502482160925865 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.2722082138061523\n",
      "\t grad: 2.0 3.0 -0.987055778503418\n",
      "\t grad: 3.0 4.0 0.9567947387695312\n",
      "progress: 7 0.025429338216781616 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.2461588382720947\n",
      "\t grad: 2.0 3.0 -0.8849430084228516\n",
      "\t grad: 3.0 4.0 1.1681671142578125\n",
      "progress: 8 0.03790595754981041 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.226900339126587\n",
      "\t grad: 2.0 3.0 -0.8094491958618164\n",
      "\t grad: 3.0 4.0 1.324441909790039\n",
      "progress: 9 0.04872628673911095 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.2126619815826416\n",
      "\t grad: 2.0 3.0 -0.7536354064941406\n",
      "\t grad: 3.0 4.0 1.4399757385253906\n",
      "progress: 10 0.0575980581343174 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.2021355628967285\n",
      "\t grad: 2.0 3.0 -0.712371826171875\n",
      "\t grad: 3.0 4.0 1.5253887176513672\n",
      "progress: 11 0.06463363021612167 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1943533420562744\n",
      "\t grad: 2.0 3.0 -0.6818647384643555\n",
      "\t grad: 3.0 4.0 1.5885400772094727\n",
      "progress: 12 0.07009609788656235 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1885995864868164\n",
      "\t grad: 2.0 3.0 -0.6593103408813477\n",
      "\t grad: 3.0 4.0 1.6352291107177734\n",
      "progress: 13 0.07427706569433212 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1843459606170654\n",
      "\t grad: 2.0 3.0 -0.6426362991333008\n",
      "\t grad: 3.0 4.0 1.6697416305541992\n",
      "progress: 14 0.07744547724723816 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1812012195587158\n",
      "\t grad: 2.0 3.0 -0.6303091049194336\n",
      "\t grad: 3.0 4.0 1.6952590942382812\n",
      "progress: 15 0.07983064651489258 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1788763999938965\n",
      "\t grad: 2.0 3.0 -0.6211957931518555\n",
      "\t grad: 3.0 4.0 1.7141246795654297\n",
      "progress: 16 0.0816173180937767 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1771574020385742\n",
      "\t grad: 2.0 3.0 -0.6144571304321289\n",
      "\t grad: 3.0 4.0 1.728072166442871\n",
      "progress: 17 0.08295092731714249 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.175886631011963\n",
      "\t grad: 2.0 3.0 -0.6094751358032227\n",
      "\t grad: 3.0 4.0 1.7383861541748047\n",
      "progress: 18 0.08394406735897064 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1749470233917236\n",
      "\t grad: 2.0 3.0 -0.6057920455932617\n",
      "\t grad: 3.0 4.0 1.7460107803344727\n",
      "progress: 19 0.08468204736709595 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1742522716522217\n",
      "\t grad: 2.0 3.0 -0.6030693054199219\n",
      "\t grad: 3.0 4.0 1.7516469955444336\n",
      "progress: 20 0.08522964268922806 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.173738956451416\n",
      "\t grad: 2.0 3.0 -0.6010570526123047\n",
      "\t grad: 3.0 4.0 1.7558097839355469\n",
      "progress: 21 0.0856352224946022 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1733593940734863\n",
      "\t grad: 2.0 3.0 -0.5995683670043945\n",
      "\t grad: 3.0 4.0 1.7588939666748047\n",
      "progress: 22 0.08593633025884628 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1730787754058838\n",
      "\t grad: 2.0 3.0 -0.5984687805175781\n",
      "\t grad: 3.0 4.0 1.7611684799194336\n",
      "progress: 23 0.08615873008966446 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1728713512420654\n",
      "\t grad: 2.0 3.0 -0.5976552963256836\n",
      "\t grad: 3.0 4.0 1.7628536224365234\n",
      "progress: 24 0.08632369339466095 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.172717809677124\n",
      "\t grad: 2.0 3.0 -0.5970535278320312\n",
      "\t grad: 3.0 4.0 1.7640981674194336\n",
      "progress: 25 0.08644562214612961 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1726043224334717\n",
      "\t grad: 2.0 3.0 -0.5966091156005859\n",
      "\t grad: 3.0 4.0 1.765019416809082\n",
      "progress: 26 0.08653593063354492 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.172520637512207\n",
      "\t grad: 2.0 3.0 -0.5962810516357422\n",
      "\t grad: 3.0 4.0 1.7656974792480469\n",
      "progress: 27 0.08660243451595306 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1724584102630615\n",
      "\t grad: 2.0 3.0 -0.5960369110107422\n",
      "\t grad: 3.0 4.0 1.7662038803100586\n",
      "progress: 28 0.08665211498737335 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.172412633895874\n",
      "\t grad: 2.0 3.0 -0.5958576202392578\n",
      "\t grad: 3.0 4.0 1.7665729522705078\n",
      "progress: 29 0.086688332259655 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1723787784576416\n",
      "\t grad: 2.0 3.0 -0.5957250595092773\n",
      "\t grad: 3.0 4.0 1.7668476104736328\n",
      "progress: 30 0.08671528846025467 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.172353744506836\n",
      "\t grad: 2.0 3.0 -0.5956268310546875\n",
      "\t grad: 3.0 4.0 1.7670536041259766\n",
      "progress: 31 0.08673550933599472 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.172335147857666\n",
      "\t grad: 2.0 3.0 -0.5955533981323242\n",
      "\t grad: 3.0 4.0 1.7672052383422852\n",
      "progress: 32 0.08675039559602737 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1723213195800781\n",
      "\t grad: 2.0 3.0 -0.5954999923706055\n",
      "\t grad: 3.0 4.0 1.7673139572143555\n",
      "progress: 33 0.08676107227802277 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1723113059997559\n",
      "\t grad: 2.0 3.0 -0.5954599380493164\n",
      "\t grad: 3.0 4.0 1.7673969268798828\n",
      "progress: 34 0.0867692157626152 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1723036766052246\n",
      "\t grad: 2.0 3.0 -0.5954303741455078\n",
      "\t grad: 3.0 4.0 1.7674598693847656\n",
      "progress: 35 0.08677539974451065 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722981929779053\n",
      "\t grad: 2.0 3.0 -0.5954093933105469\n",
      "\t grad: 3.0 4.0 1.767502784729004\n",
      "progress: 36 0.08677961677312851 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722941398620605\n",
      "\t grad: 2.0 3.0 -0.595393180847168\n",
      "\t grad: 3.0 4.0 1.7675342559814453\n",
      "progress: 37 0.08678270131349564 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722912788391113\n",
      "\t grad: 2.0 3.0 -0.5953817367553711\n",
      "\t grad: 3.0 4.0 1.7675600051879883\n",
      "progress: 38 0.08678523451089859 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722891330718994\n",
      "\t grad: 2.0 3.0 -0.5953731536865234\n",
      "\t grad: 3.0 4.0 1.7675800323486328\n",
      "progress: 39 0.08678720146417618 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722872257232666\n",
      "\t grad: 2.0 3.0 -0.5953655242919922\n",
      "\t grad: 3.0 4.0 1.767594337463379\n",
      "progress: 40 0.0867886021733284 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.172286033630371\n",
      "\t grad: 2.0 3.0 -0.5953617095947266\n",
      "\t grad: 3.0 4.0 1.7676029205322266\n",
      "progress: 41 0.08678944408893585 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722850799560547\n",
      "\t grad: 2.0 3.0 -0.5953578948974609\n",
      "\t grad: 3.0 4.0 1.767608642578125\n",
      "progress: 42 0.08679001033306122 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722846031188965\n",
      "\t grad: 2.0 3.0 -0.5953559875488281\n",
      "\t grad: 3.0 4.0 1.7676143646240234\n",
      "progress: 43 0.08679056912660599 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722841262817383\n",
      "\t grad: 2.0 3.0 -0.5953540802001953\n",
      "\t grad: 3.0 4.0 1.7676172256469727\n",
      "progress: 44 0.08679085224866867 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.17228364944458\n",
      "\t grad: 2.0 3.0 -0.5953521728515625\n",
      "\t grad: 3.0 4.0 1.7676200866699219\n",
      "progress: 45 0.08679113537073135 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.172283411026001\n",
      "\t grad: 2.0 3.0 -0.5953512191772461\n",
      "\t grad: 3.0 4.0 1.767622947692871\n",
      "progress: 46 0.08679141104221344 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722831726074219\n",
      "\t grad: 2.0 3.0 -0.5953502655029297\n",
      "\t grad: 3.0 4.0 1.7676258087158203\n",
      "progress: 47 0.08679169416427612 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722829341888428\n",
      "\t grad: 2.0 3.0 -0.5953493118286133\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 48 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 49 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 50 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 51 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 52 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 53 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 54 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 55 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 56 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 57 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 58 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 59 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 60 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 61 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 62 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 63 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 64 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 65 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 66 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 67 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 68 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 69 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 70 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 71 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 72 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 73 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 74 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 75 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 76 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 77 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 78 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 79 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 80 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 81 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 82 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 83 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 84 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 85 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 86 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 87 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 88 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 89 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 90 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 91 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 92 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 93 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 94 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 95 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 96 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 97 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 98 0.0867919772863388 \n",
      "\n",
      "\t grad: 1.0 2.0 -1.1722826957702637\n",
      "\t grad: 2.0 3.0 -0.5953483581542969\n",
      "\t grad: 3.0 4.0 1.7676286697387695\n",
      "progress: 99 0.0867919772863388 \n",
      "\n",
      "predict (after training) 4 5.655434608459473\n"
     ]
    }
   ],
   "source": [
    "# 如何在PyTorch中构建模型？\n",
    "import torch\n",
    "\n",
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 3.0, 4.0]\n",
    "w = torch.tensor([1.0]) #w就一个值就是1.0\n",
    "w.requires_grad = True #他是需要计算梯度的\n",
    "\n",
    "def forward(x):\n",
    "    return x * w\n",
    "\n",
    "def loss(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y) ** 2\n",
    "\n",
    "print(\"predict (before training)\", 4, forward(4).item())\n",
    "for epoch in range(100):\n",
    "    for x, y in zip(x_data, y_data):\n",
    "        l = loss(x, y)\n",
    "        l.backward()\n",
    "        #注意损失值为l.item()； 如果想计算所有损失值的和就用sum += l.item()\n",
    "        print(\"\\t grad:\" , x, y, w.grad.item())\n",
    "        w.data = w.data - 0.01 * w.grad.data\n",
    "        w.grad.data.zero_()#把梯度中的数据全部清零\n",
    "    print(\"progress:\", epoch, l.item(),\"\\n\")\n",
    "print(\"predict (after training)\", 4, forward(4).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training: 4 4.0\n",
      "\t grad: -2.0\n",
      "\t grad: -7.840000152587891\n",
      "\t grad: -16.228801727294922\n",
      "\t progress:\n",
      " epoch: 0 loss: 7.315943717956543 \n",
      "\n",
      "\t grad: -1.478623867034912\n",
      "\t grad: -5.796205520629883\n",
      "\t grad: -11.998146057128906\n",
      "\t progress:\n",
      " epoch: 1 loss: 3.9987640380859375 \n",
      "\n",
      "\t grad: -1.0931644439697266\n",
      "\t grad: -4.285204887390137\n",
      "\t grad: -8.870372772216797\n",
      "\t progress:\n",
      " epoch: 2 loss: 2.1856532096862793 \n",
      "\n",
      "\t grad: -0.8081896305084229\n",
      "\t grad: -3.1681032180786133\n",
      "\t grad: -6.557973861694336\n",
      "\t progress:\n",
      " epoch: 3 loss: 1.1946394443511963 \n",
      "\n",
      "\t grad: -0.5975041389465332\n",
      "\t grad: -2.3422164916992188\n",
      "\t grad: -4.848389625549316\n",
      "\t progress:\n",
      " epoch: 4 loss: 0.6529689431190491 \n",
      "\n",
      "\t grad: -0.4417421817779541\n",
      "\t grad: -1.7316293716430664\n",
      "\t grad: -3.58447265625\n",
      "\t progress:\n",
      " epoch: 5 loss: 0.35690122842788696 \n",
      "\n",
      "\t grad: -0.3265852928161621\n",
      "\t grad: -1.2802143096923828\n",
      "\t grad: -2.650045394897461\n",
      "\t progress:\n",
      " epoch: 6 loss: 0.195076122879982 \n",
      "\n",
      "\t grad: -0.24144840240478516\n",
      "\t grad: -0.9464778900146484\n",
      "\t grad: -1.9592113494873047\n",
      "\t progress:\n",
      " epoch: 7 loss: 0.10662525147199631 \n",
      "\n",
      "\t grad: -0.17850565910339355\n",
      "\t grad: -0.699742317199707\n",
      "\t grad: -1.4484672546386719\n",
      "\t progress:\n",
      " epoch: 8 loss: 0.0582793727517128 \n",
      "\n",
      "\t grad: -0.1319713592529297\n",
      "\t grad: -0.5173273086547852\n",
      "\t grad: -1.070866584777832\n",
      "\t progress:\n",
      " epoch: 9 loss: 0.03185431286692619 \n",
      "\n",
      "\t grad: -0.09756779670715332\n",
      "\t grad: -0.3824653625488281\n",
      "\t grad: -0.7917022705078125\n",
      "\t progress:\n",
      " epoch: 10 loss: 0.017410902306437492 \n",
      "\n",
      "\t grad: -0.07213282585144043\n",
      "\t grad: -0.2827606201171875\n",
      "\t grad: -0.5853137969970703\n",
      "\t progress:\n",
      " epoch: 11 loss: 0.009516451507806778 \n",
      "\n",
      "\t grad: -0.053328514099121094\n",
      "\t grad: -0.2090473175048828\n",
      "\t grad: -0.43272972106933594\n",
      "\t progress:\n",
      " epoch: 12 loss: 0.005201528314501047 \n",
      "\n",
      "\t grad: -0.039426326751708984\n",
      "\t grad: -0.15455150604248047\n",
      "\t grad: -0.3199195861816406\n",
      "\t progress:\n",
      " epoch: 13 loss: 0.0028430151287466288 \n",
      "\n",
      "\t grad: -0.029148340225219727\n",
      "\t grad: -0.11426162719726562\n",
      "\t grad: -0.23652076721191406\n",
      "\t progress:\n",
      " epoch: 14 loss: 0.0015539465239271522 \n",
      "\n",
      "\t grad: -0.021549701690673828\n",
      "\t grad: -0.08447456359863281\n",
      "\t grad: -0.17486286163330078\n",
      "\t progress:\n",
      " epoch: 15 loss: 0.0008493617060594261 \n",
      "\n",
      "\t grad: -0.01593184471130371\n",
      "\t grad: -0.062453269958496094\n",
      "\t grad: -0.12927818298339844\n",
      "\t progress:\n",
      " epoch: 16 loss: 0.00046424579340964556 \n",
      "\n",
      "\t grad: -0.011778593063354492\n",
      "\t grad: -0.046172142028808594\n",
      "\t grad: -0.09557533264160156\n",
      "\t progress:\n",
      " epoch: 17 loss: 0.0002537401160225272 \n",
      "\n",
      "\t grad: -0.00870823860168457\n",
      "\t grad: -0.03413581848144531\n",
      "\t grad: -0.07066154479980469\n",
      "\t progress:\n",
      " epoch: 18 loss: 0.00013869594840798527 \n",
      "\n",
      "\t grad: -0.006437778472900391\n",
      "\t grad: -0.025236129760742188\n",
      "\t grad: -0.052239418029785156\n",
      "\t progress:\n",
      " epoch: 19 loss: 7.580435340059921e-05 \n",
      "\n",
      "\t grad: -0.004759550094604492\n",
      "\t grad: -0.018657684326171875\n",
      "\t grad: -0.038620948791503906\n",
      "\t progress:\n",
      " epoch: 20 loss: 4.143271507928148e-05 \n",
      "\n",
      "\t grad: -0.003518819808959961\n",
      "\t grad: -0.0137939453125\n",
      "\t grad: -0.028553009033203125\n",
      "\t progress:\n",
      " epoch: 21 loss: 2.264650902361609e-05 \n",
      "\n",
      "\t grad: -0.00260162353515625\n",
      "\t grad: -0.010198593139648438\n",
      "\t grad: -0.021108627319335938\n",
      "\t progress:\n",
      " epoch: 22 loss: 1.2377059647405986e-05 \n",
      "\n",
      "\t grad: -0.0019233226776123047\n",
      "\t grad: -0.0075397491455078125\n",
      "\t grad: -0.0156097412109375\n",
      "\t progress:\n",
      " epoch: 23 loss: 6.768445018678904e-06 \n",
      "\n",
      "\t grad: -0.0014221668243408203\n",
      "\t grad: -0.0055751800537109375\n",
      "\t grad: -0.011541366577148438\n",
      "\t progress:\n",
      " epoch: 24 loss: 3.7000872907810844e-06 \n",
      "\n",
      "\t grad: -0.0010514259338378906\n",
      "\t grad: -0.0041217803955078125\n",
      "\t grad: -0.008531570434570312\n",
      "\t progress:\n",
      " epoch: 25 loss: 2.021880391112063e-06 \n",
      "\n",
      "\t grad: -0.0007772445678710938\n",
      "\t grad: -0.0030469894409179688\n",
      "\t grad: -0.006305694580078125\n",
      "\t progress:\n",
      " epoch: 26 loss: 1.1044940038118511e-06 \n",
      "\n",
      "\t grad: -0.0005745887756347656\n",
      "\t grad: -0.0022525787353515625\n",
      "\t grad: -0.0046634674072265625\n",
      "\t progress:\n",
      " epoch: 27 loss: 6.041091182851233e-07 \n",
      "\n",
      "\t grad: -0.0004248619079589844\n",
      "\t grad: -0.0016651153564453125\n",
      "\t grad: -0.003444671630859375\n",
      "\t progress:\n",
      " epoch: 28 loss: 3.296045179013163e-07 \n",
      "\n",
      "\t grad: -0.0003139972686767578\n",
      "\t grad: -0.0012311935424804688\n",
      "\t grad: -0.0025491714477539062\n",
      "\t progress:\n",
      " epoch: 29 loss: 1.805076408345485e-07 \n",
      "\n",
      "\t grad: -0.00023221969604492188\n",
      "\t grad: -0.0009107589721679688\n",
      "\t grad: -0.0018854141235351562\n",
      "\t progress:\n",
      " epoch: 30 loss: 9.874406714516226e-08 \n",
      "\n",
      "\t grad: -0.00017189979553222656\n",
      "\t grad: -0.0006742477416992188\n",
      "\t grad: -0.00139617919921875\n",
      "\t progress:\n",
      " epoch: 31 loss: 5.4147676564753056e-08 \n",
      "\n",
      "\t grad: -0.0001270771026611328\n",
      "\t grad: -0.0004978179931640625\n",
      "\t grad: -0.00102996826171875\n",
      "\t progress:\n",
      " epoch: 32 loss: 2.9467628337442875e-08 \n",
      "\n",
      "\t grad: -9.393692016601562e-05\n",
      "\t grad: -0.0003681182861328125\n",
      "\t grad: -0.0007610321044921875\n",
      "\t progress:\n",
      " epoch: 33 loss: 1.6088051779661328e-08 \n",
      "\n",
      "\t grad: -6.937980651855469e-05\n",
      "\t grad: -0.00027179718017578125\n",
      "\t grad: -0.000560760498046875\n",
      "\t progress:\n",
      " epoch: 34 loss: 8.734787115827203e-09 \n",
      "\n",
      "\t grad: -5.125999450683594e-05\n",
      "\t grad: -0.00020122528076171875\n",
      "\t grad: -0.0004177093505859375\n",
      "\t progress:\n",
      " epoch: 35 loss: 4.8466972657479346e-09 \n",
      "\n",
      "\t grad: -3.790855407714844e-05\n",
      "\t grad: -0.000148773193359375\n",
      "\t grad: -0.000308990478515625\n",
      "\t progress:\n",
      " epoch: 36 loss: 2.6520865503698587e-09 \n",
      "\n",
      "\t grad: -2.8133392333984375e-05\n",
      "\t grad: -0.000110626220703125\n",
      "\t grad: -0.0002288818359375\n",
      "\t progress:\n",
      " epoch: 37 loss: 1.4551915228366852e-09 \n",
      "\n",
      "\t grad: -2.09808349609375e-05\n",
      "\t grad: -8.20159912109375e-05\n",
      "\t grad: -0.00016880035400390625\n",
      "\t progress:\n",
      " epoch: 38 loss: 7.914877642178908e-10 \n",
      "\n",
      "\t grad: -1.5497207641601562e-05\n",
      "\t grad: -6.103515625e-05\n",
      "\t grad: -0.000125885009765625\n",
      "\t progress:\n",
      " epoch: 39 loss: 4.4019543565809727e-10 \n",
      "\n",
      "\t grad: -1.1444091796875e-05\n",
      "\t grad: -4.482269287109375e-05\n",
      "\t grad: -9.1552734375e-05\n",
      "\t progress:\n",
      " epoch: 40 loss: 2.3283064365386963e-10 \n",
      "\n",
      "\t grad: -8.344650268554688e-06\n",
      "\t grad: -3.24249267578125e-05\n",
      "\t grad: -6.580352783203125e-05\n",
      "\t progress:\n",
      " epoch: 41 loss: 1.2028067430946976e-10 \n",
      "\n",
      "\t grad: -5.9604644775390625e-06\n",
      "\t grad: -2.288818359375e-05\n",
      "\t grad: -4.57763671875e-05\n",
      "\t progress:\n",
      " epoch: 42 loss: 5.820766091346741e-11 \n",
      "\n",
      "\t grad: -4.291534423828125e-06\n",
      "\t grad: -1.71661376953125e-05\n",
      "\t grad: -3.719329833984375e-05\n",
      "\t progress:\n",
      " epoch: 43 loss: 3.842615114990622e-11 \n",
      "\n",
      "\t grad: -3.337860107421875e-06\n",
      "\t grad: -1.33514404296875e-05\n",
      "\t grad: -2.86102294921875e-05\n",
      "\t progress:\n",
      " epoch: 44 loss: 2.2737367544323206e-11 \n",
      "\n",
      "\t grad: -2.6226043701171875e-06\n",
      "\t grad: -1.049041748046875e-05\n",
      "\t grad: -2.288818359375e-05\n",
      "\t progress:\n",
      " epoch: 45 loss: 1.4551915228366852e-11 \n",
      "\n",
      "\t grad: -1.9073486328125e-06\n",
      "\t grad: -7.62939453125e-06\n",
      "\t grad: -1.430511474609375e-05\n",
      "\t progress:\n",
      " epoch: 46 loss: 5.6843418860808015e-12 \n",
      "\n",
      "\t grad: -1.430511474609375e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t grad: -1.1444091796875e-05\n",
      "\t progress:\n",
      " epoch: 47 loss: 3.637978807091713e-12 \n",
      "\n",
      "\t grad: -1.1920928955078125e-06\n",
      "\t grad: -4.76837158203125e-06\n",
      "\t grad: -1.1444091796875e-05\n",
      "\t progress:\n",
      " epoch: 48 loss: 3.637978807091713e-12 \n",
      "\n",
      "\t grad: -9.5367431640625e-07\n",
      "\t grad: -3.814697265625e-06\n",
      "\t grad: -8.58306884765625e-06\n",
      "\t progress:\n",
      " epoch: 49 loss: 2.0463630789890885e-12 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 50 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 51 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 52 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 53 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 54 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 55 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 56 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 57 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 58 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 59 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 60 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 61 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 62 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 63 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 64 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 65 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 66 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 67 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 68 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 69 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 70 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 71 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 72 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 73 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 74 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 75 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 76 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 77 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 78 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 79 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 80 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 81 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 82 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 83 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 84 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 85 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 86 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 87 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 88 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 89 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 90 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 91 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 92 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 93 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 94 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 95 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 96 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 97 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 98 loss: 9.094947017729282e-13 \n",
      "\n",
      "\t grad: -7.152557373046875e-07\n",
      "\t grad: -2.86102294921875e-06\n",
      "\t grad: -5.7220458984375e-06\n",
      "\t progress:\n",
      " epoch: 99 loss: 9.094947017729282e-13 \n",
      "\n",
      "predict (after training) 4 7.999998569488525\n"
     ]
    }
   ],
   "source": [
    "# Experience first time torch model \n",
    "import torch \n",
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "w = torch.tensor([1.0])\n",
    "w.requires_grad = True\n",
    "def forward(x):\n",
    "    return x * w\n",
    "\n",
    "def loss(x, y):\n",
    "    loss = (forward(x) - y) ** 2\n",
    "    return loss\n",
    "\n",
    "print(\"Before training:\", 4, forward(4).item())\n",
    "for epoch in range(100):\n",
    "    for x, y in zip(x_data, y_data):\n",
    "        l = loss(x, y)\n",
    "        l.backward()\n",
    "        print(\"\\t grad:\", w.grad.item())\n",
    "        w.data = w.data - 0.01 * w.grad.data\n",
    "        w.grad.data.zero_()\n",
    "    print(\"\\t progress:\\n\", \"epoch:\",epoch, \"loss:\", l.item(), \"\\n\")\n",
    "print(\"predict (after training)\", 4, forward(4).item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9bf260bc314943363efc9020905b20dab6f5821c4b3cc71f172f4bac0be09ec5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
